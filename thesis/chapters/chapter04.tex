% Continuing from the findings in Chapter \ref{chp2}, the 9H anchor configuration
% was used to perform a preliminary classification of activities in the 
% kitchen. Steps performed in making a sandwich were broken down and organized
% into Setup, Preparation, Cooking, and Finishing steps, Figure \ref{fig:sandwichbreakdown}. 

% \begin{figure}[ht]
%     \centering
%     \includegraphics*[width=\textwidth]{makingsandwich}
%     \caption{Task decomposition of making a sandwich.}
%     \label{fig:sandwichbreakdown}
% \end{figure}

% From the actions shown in Figure \ref{fig:sandwichbreakdown}, actions 
% with distinct location or patterns were selected as classes for classification.
% OPENFRIDGE, OPENFREEZER, and GETPLATE were selected as classes from the Setup
% category, washing hands/vegetables/fruits/using the kitchen sink were grouped 
% into a WASHHANDS category, and SLICETOMATO were selected as a class. Finally,
% All intermediary transitions or motionless segments were grouped into a 
% UNDEFINED category. Single trials were performed to collect data for each of 
% these classes. 

% \clearpage
% \section{Experimental Protocol}
% \subsection{Setup}
% Several points were enforced to ensure that the training dataset captures
% the variation in action sufficiently when classifying data from right-handed individuals.

% \begin{itemize}
%     \item Pozyx Tag is mounted on the right wrist (Figure \ref{fig:wristsensor}).
%     \item Initial position for each of the single trials are not marked. Participant
%     will be able to choose a location from which they can perform the action comfortably
%     without moving their feet.
%     \item An action starts when the individual contacts the appliance or furniture.
%     For SLICETOMATO the action starts when an individual starts slicing the 
%     tomato and ends when they stop slicing the tomato. Motions such as picking up the
%     knife and getting in position to slice were considered transitions and labelled 
%     as UNDEFINED. 
% \end{itemize}


% \begin{figure}[ht]
%     \centering
%     \includegraphics*[width=0.4\textwidth]{wristsensor}
%     \caption{Pozyx tag mounted on the wrist. The participant is performing the 
%     OPENFRIDGE task
%     }
%     \label{fig:wristsensor}
% \end{figure}

% \subsection{Data Collection}
% Custom Python stopwatch scripts were created to accurately label periods of 
% transitions (quiet standing + getting into position for the action) and the action.
% An example of the data collected is shown in Figure \ref{fig:openfridgedata}. 
% For each action there is a quiet standing period at the beginning and end. 
% OPENFRIDGE, OPENFREEZER, OPENPLATE, WASHHANDS each had 5 repetitions for each trial.
% SLICETOMATO contained 3 slices to conserve the amount of tomato.
% Each action had a total of 5 trials. 

% \begin{figure}[ht]
%     \centering
%     \includegraphics*[width=\textwidth]{openfridgedata}
%     \caption{Labelled position data of the OPENFRIDGE action. Note that the "quiet standing"
%     periods do not consist entirely of quiet standing, but also include traces of 
%     transitions from getting into the correct position to perform the action. 
%     }
%     \label{fig:openfridgedata}
% \end{figure}

% Since the Pozyx Tag contained a BNO055 chip, in addition to 3D Position data, the
% tags were able to capture inertial data including Accelerometer Data, 
% Linear Accelerometer Data, Angular Velocity Data, and the orientation.

% Data for each of the actions that relate to making a sandwich were collected from 
% 2 participants.

% \section{Feature Extraction}
% An initial sliding window with a width of 2 seconds and a stride length of 1 second
% was used to ensure that enough feature vectors could be extracted from the SLICETOMATO dataset.
% An example of a windows taken for the OPENFRIDGE action and UNDEFINED action 
% are shown in Figure \ref{fig:windows}.

% \begin{figure}[ht]
%     \centering
%     \begin{subfigure}{\textwidth}
%         \includegraphics*[width=\textwidth]{windowing1}
%         \caption{}
%     \end{subfigure}
%     \begin{subfigure}{\textwidth}
%         \includegraphics*[width=\textwidth]{windowing3}
%         \caption{}
%     \end{subfigure}
%     \caption{Obtaining windows from the OPENFRIDGE dataset. The green vertical
%     lines section off a 2-second window. (a) A window
%     labelled OPENFRIDGE. (b) A window labelled UNDEFINED.
%     }
%     \label{fig:windows}
% \end{figure}

% From each window, basic statistical measures over the entire window were taken. These
% measures include the MEAN, MEDIAN, MODE (to 5cm for position), MAX, MIN, and STD
% of the entire window. 
% From each window of data, there were a total of 3 (axes) * 5 (types of data) * 6 (statistical measures) = 90 Features 

% From the entire timeseries dataset, 2773 feature vectors were extracted.
% Refer to Table \ref{tab:countsfeatvect} for the breakdown of counts for each label.

% \begin{table}[!htbp]
%   \centering
%   \caption{Count of the occurrences of each action.}
%   \label{tab:countsfeatvect}
%   \begin{tabular}{ll}
%     \toprule
%     \thead{Action} & \thead{Count} \\
%     \midrule
%     UNDEFINED   & 1518 \\
%     SLICETOMATO & 197 \\
%     WAHSHANDS   & 316 \\
%     OPENFRIDGE  & 239 \\
%     OPENFREEZER & 203 \\
%     GETPLATE    & 300 \\
%     \bottomrule
%   \end{tabular}
% \end{table}

% \clearpage
% \section{Model Selection}
% A 60:40 split was used to train and test the model selected. Several models 
% were chosen including Linear Support Vector Machine, Radial Support Vector Machine,
% K-Nearest Neighbors, Decision Trees and Random Forests. As this was a pilot study in determining the 
% feasibility of classification of the fine-grained actions involved
% in making a sandwich, rigorous parameter tuning and feature selection 
% were neglected and the defaults from the sklearn Python package were used.

% \section{Results}
% The confusion matrices from each model are output in Figures \ref{fig:cm-svm-lin}-\ref{fig:cm-rf}.
% Total accuracy was reported as well as the sensitivity, specificity, and precision of each class 
% were reported. These measures are calculated as follows:

% \begin{equation}
%     \text{Accuracy} = \frac{\text{All} \: TP}{N}
% \end{equation}

% \begin{equation}
%     \text{Sensitivity} = \frac{TP}{TP + FN}
% \end{equation}

% \begin{equation}
%     \text{Precision} = \frac{TP}{TP + FP}
% \end{equation}

% \begin{equation}
%     \text{Specificity} = \frac{TN}{TN + FP}
% \end{equation}
% Where $N$ is the number of samples $TP$ are True Positives, 
% $TN$ are true negatives, $FP$ are false positives and $FN$ are false negatives.

% \begin{figure}[ht]
%     \centering
%     \includegraphics*[height=0.8\textheight]{cm-svm-lin}
%     \caption{Test confusion matrix using the Support Vector Classifier with a Linear Kernel}
%     \label{fig:cm-svm-lin}
% \end{figure}


% \begin{figure}[ht]
%     \centering
%     \includegraphics*[height=0.8\textheight]{cm-svm-rbf}
%     \caption{Test confusion matrix using the Support Vector Classifier with a Radial Kernel}
%     \label{fig:cm-svm-rbf}
% \end{figure}

% \begin{figure}[ht]
%     \centering
%     \includegraphics*[height=0.8\textheight]{cm-svm-poly}
%     \caption{Test confusion matrix using the Support Vector Classifier with a Polynomial Kernel}
%     \label{fig:cm-svm-poly}
% \end{figure}

% \begin{figure}[ht]
%     \centering
%     \includegraphics*[height=0.8\textheight]{cm-5nn}
%     \caption{Test confusion matrix using the K-Nearest Neighbors Classifier}
%     \label{fig:cm-5nn}
% \end{figure}

% \begin{figure}[ht]
%     \centering
%     \includegraphics*[height=0.8\textheight]{cm-decision}
%     \caption{Test confusion matrix using the Decision Tree Classifier}
%     \label{fig:cm-decision}
% \end{figure}

% \begin{figure}[ht]
%     \centering
%     \includegraphics*[height=0.8\textheight]{cm-rf}
%     \caption{Test confusion matrix using the Random Forests Classifier}
%     \label{fig:cm-rf}
% \end{figure}

% \clearpage
% \section{Discussion}
% Table \ref{tab:accuracies} summarizes the accuracies obtained from each model.

% \begin{table}[!htbp]
%   \centering
%   \caption{Accuracy of each Model}
%   \label{tab:accuracies}
%   \begin{tabular}{ll}
%     \toprule
%     \thead{Model Name} & \thead{Accuracy (\%)} \\
%     \midrule
%     SVM Linear      & 97.7 \\
%     SVM Radial      & 88.1 \\
%     SVM Polynomial  & 93.1 \\
%     kNN             & 97.4 \\
%     Decision Tree   & 97.5 \\
%     Random Forests  & 99.3 \\
%     \bottomrule
%   \end{tabular}
% \end{table}

% With the exception of the Radial SVM, all of the models perform well
% achieving an accuracy of somewhere in the high 90s. In day-to-day
% activities, there is a disproportionately higher number of the 
% UNDEFINED class compared to the other "action" classes signifying the 
% presence of class imbalance. If a classifier 
% guesses all UNDEFINED it can obtain an accuracy of 599/1110 = 54\%. Thus,
% accuracies taken around 54\% should be interpreted with caution. Other 
% metrics such as the Sensitivity, Precision and Specificity have been provided
% to address this class imbalance. Sensitivity is the rate at which the classifier
% predicts a $TP$, Precision is the fraction of predictions that are actually
% true, and Specificity is the rate at which the classifier predicts a $TN$.
% Of all the models, the Random Forests Classifier at the default settings
% seem to the best in terms of Accuracy and Precision, Sensitivity, and Specificity
% for all classes. 

% The performance of these models in the real-time will need to be tested 
% and quantified before any conclusions can be made. A high accuracy 
% is promising, but may also be indicative of overfitting which means 
% that the model will not be able to generalize variation experienced in the 
% real world. In later sections, more fine-grained actions will be considered,
% models will be more rigorously tuned, and the performance in real-time
% will be investigated.

